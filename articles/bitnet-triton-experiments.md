---
title: "BitNet ã‚’ GPU ã§å®Ÿè£…ã—ã¦åˆ†ã‹ã£ãŸã“ã¨ - Triton ã‚«ãƒ¼ãƒãƒ«é–‹ç™ºã‹ã‚‰å®Ÿç”¨æ€§æ¤œè¨¼ã¾ã§"
emoji: "ğŸ”¬"
type: "tech"
topics: ["bitnet", "triton", "pytorch", "gpu", "llm"]
published: false
---

## ã¯ã˜ã‚ã«

BitNet ã¯ Microsoft ãŒææ¡ˆã—ãŸ 1.58-bit é‡å­åŒ–æ‰‹æ³•ã§ã€é‡ã¿ã‚’ {-1, 0, +1} ã®3å€¤ã«åˆ¶é™ã™ã‚‹ã“ã¨ã§ã€å¤§å¹…ãªãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã¨é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ã€BitNet ã‚’ GPU ä¸Šã§ Triton ã‚«ãƒ¼ãƒãƒ«ã¨ã—ã¦å®Ÿè£…ã—ã€æ§˜ã€…ãªå®Ÿé¨“ã‚’é€šã˜ã¦å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹ã‚’å…±æœ‰ã—ã¾ã™ã€‚

**å®Ÿé¨“ã§ä½œæˆã—ãŸãƒªãƒã‚¸ãƒˆãƒª:**
- [bitnet-triton](https://github.com/yukikotani/bitnet-triton) - Triton ã‚«ãƒ¼ãƒãƒ«å®Ÿè£…
- [bitnet-mnist](https://github.com/yukikotani/bitnet-mnist) - MNIST ã§ã®æ¤œè¨¼

## BitNet ã¨ã¯

### åŸºæœ¬æ¦‚å¿µ

BitNet b1.58 ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ã‚’3å€¤ {-1, 0, +1} ã«é‡å­åŒ–ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚

```python
# é€šå¸¸ã® Linear
y = x @ W  # W ã¯ FP32/FP16

# BitNet Linear
W_ternary = quantize(W)  # W âˆˆ {-1, 0, +1}
y = x @ W_ternary * scale
```

### ãƒ¡ãƒ¢ãƒªåŠ¹ç‡

| ç²¾åº¦ | ãƒ“ãƒƒãƒˆæ•° | åœ§ç¸®ç‡ |
|------|---------|-------|
| FP32 | 32 bit | 1x |
| FP16 | 16 bit | 2x |
| INT8 | 8 bit | 4x |
| INT4 | 4 bit | 8x |
| **BitNet (2-bit)** | 2 bit | **16x** |

3å€¤ã‚’2ãƒ“ãƒƒãƒˆã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€16å€‹ã®é‡ã¿ã‚’1ã¤ã® int32 ã«ãƒ‘ãƒƒã‚­ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€FP32 æ¯”ã§ 16å€ã®ãƒ¡ãƒ¢ãƒªåœ§ç¸®ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## Triton ã‚«ãƒ¼ãƒãƒ«ã®å®Ÿè£…

### 2-bit ãƒ‘ãƒƒã‚­ãƒ³ã‚°

```python
def pack_weights(weight: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
    """FP32 é‡ã¿ã‚’ 2-bit ã«ãƒ‘ãƒƒã‚¯"""
    # ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—
    scale = weight.abs().mean(dim=1, keepdim=True).clamp(min=1e-5)

    # é‡å­åŒ–: {-1, 0, +1}
    w_scaled = weight / scale
    w_ternary = torch.clamp(torch.round(w_scaled), -1, 1).to(torch.int8)

    # {0, 1, 2} ã«ãƒãƒƒãƒ”ãƒ³ã‚°
    w_mapped = (w_ternary + 1).to(torch.uint8)

    # 16å€‹ã®2-bitå€¤ã‚’1ã¤ã® int32 ã«ãƒ‘ãƒƒã‚¯
    packed = torch.zeros(N, K // 16, dtype=torch.int32)
    for i in range(16):
        packed |= w_reshaped[:, :, i].to(torch.int32) << (i * 2)

    return packed, scale
```

### Triton MatMul ã‚«ãƒ¼ãƒãƒ«

```python
@triton.jit
def _bitnet_matmul_kernel(
    x_ptr, packed_ptr, scale_ptr, output_ptr,
    M, N, K,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    # ãƒ–ãƒ­ãƒƒã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    pid_m, pid_n = tl.program_id(0), tl.program_id(1)

    # ã‚¢ã‚­ãƒ¥ãƒ ãƒ¬ãƒ¼ã‚¿
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    for k_start in range(0, K, BLOCK_K):
        # å…¥åŠ›ã‚’ãƒ­ãƒ¼ãƒ‰
        x = tl.load(x_ptr + ...)

        # ãƒ‘ãƒƒã‚¯ã•ã‚ŒãŸé‡ã¿ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯
        pack_idx = offs_k // 16
        bit_idx = offs_k % 16
        packed = tl.load(packed_ptr + ...)
        w_bits = (packed >> (bit_idx * 2)) & 0b11
        w = w_bits.to(tl.float32) - 1.0  # {0,1,2} -> {-1,0,+1}

        # Tensor Core ã§è¡Œåˆ—ç©
        acc += tl.dot(x, tl.trans(w), allow_tf32=True)

    # ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨
    output = acc * scales
    tl.store(output_ptr + ..., output)
```

## å®Ÿé¨“1: BitNet DiT (Diffusion Transformer)

### æ¦‚è¦

MNIST ç”»åƒç”Ÿæˆã‚¿ã‚¹ã‚¯ã§ BitNet ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚

### æœ€åˆã®å¤±æ•—: Loss ãŒ 1.0 ã§åœæ»

```python
# å¤±æ•—ã—ãŸå®Ÿè£…
class BitNetDiT(nn.Module):
    def __init__(self, ...):
        # å…¨ã¦ã‚’ BitLinear ã«
        self.time_embed = BitLinear(dim, dim)  # â† å•é¡Œ
        self.adaLN = BitLinear(dim, dim * 4)   # â† å•é¡Œ
```

**åŸå› **: æ™‚é–“åŸ‹ã‚è¾¼ã¿ã¨ AdaLNï¼ˆæ¡ä»¶ä»˜ã‘å±¤ï¼‰ã‚’ BitLinear ã«ã™ã‚‹ã¨ã€é€£ç¶šçš„ãªæ¡ä»¶æƒ…å ±ãŒé‡å­åŒ–ã§ç ´å£Šã•ã‚Œã‚‹ã€‚

### ä¿®æ­£ç‰ˆ

```python
# æˆåŠŸã—ãŸå®Ÿè£…
class BitNetDiT(nn.Module):
    def __init__(self, ...):
        # æ™‚é–“åŸ‹ã‚è¾¼ã¿ã¯é€šå¸¸ã® Linearï¼ˆæ¡ä»¶æƒ…å ±ã‚’ä¿æŒï¼‰
        self.time_embed = nn.Sequential(
            SinusoidalPositionEmbedding(dim),
            nn.Linear(dim, dim),  # â† FP32
            nn.GELU(),
            nn.Linear(dim, dim),  # â† FP32
        )

        # Attention ã¨ MLP ã¯ BitLinear OK
        self.qkv = BitLinear(dim, dim * 3)
        self.mlp = BitLinearMLP(dim)
```

### çµæœ

| ãƒ¢ãƒ‡ãƒ« | Loss (50 epoch) | ç”»åƒå“è³ª |
|--------|-----------------|---------|
| å¤±æ•—ç‰ˆ | ~1.0 (åœæ») | ãƒã‚¤ã‚º |
| ä¿®æ­£ç‰ˆ | 0.045 | èªè­˜å¯èƒ½ãªæ•°å­— |

**æ•™è¨“**: æ¡ä»¶ä»˜ã‘å±¤ï¼ˆæ™‚é–“åŸ‹ã‚è¾¼ã¿ã€ã‚¯ãƒ©ã‚¹åŸ‹ã‚è¾¼ã¿ç­‰ï¼‰ã¯é‡å­åŒ–ã—ã¦ã¯ã„ã‘ãªã„ã€‚

## å®Ÿé¨“2: LUT ãƒ™ãƒ¼ã‚¹ã‚«ãƒ¼ãƒãƒ« (BitNet.cpp æ–¹å¼)

### èƒŒæ™¯

BitNet.cpp ã¯ CPU ä¸Šã§ LUTï¼ˆLookup Tableï¼‰ã‚’ä½¿ã„ã€ä¹—ç®—ã‚’å®Œå…¨ã«æ’é™¤ã—ã¦é«˜é€ŸåŒ–ã—ã¦ã„ã¾ã™ã€‚ã“ã®æ–¹å¼ã‚’ GPU ã«ç§»æ¤ã§ãã‚‹ã‹æ¤œè¨¼ã—ã¾ã—ãŸã€‚

### T-MAC (BitNet.cpp) ã®ä»•çµ„ã¿

```
ternary weights {-1, 0, +1} ã®å ´åˆ:
  y = Î£(x where w=+1) - Î£(x where w=-1)

ä¹—ç®—ãŒä¸è¦ï¼åŠ ç®—ãƒ»æ¸›ç®—ã®ã¿ã§è¨ˆç®—å¯èƒ½

CPU ã§ã®å®Ÿè£…:
  4ã¤ã®é‡ã¿ (8bit) ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
  â†’ 256ã‚¨ãƒ³ãƒˆãƒªã® LUT ã‚’æ§‹ç¯‰
  â†’ SIMD shuffle (vpshufb) ã§é«˜é€Ÿãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—
```

### GPU ã§ã®å®Ÿè£…ã¨çµæœ

```python
# LUT ã‚¹ã‚¿ã‚¤ãƒ«ã®ã‚«ãƒ¼ãƒãƒ«
@triton.jit
def _bitnet_lut_kernel(...):
    # æ¡ä»¶åˆ†å²ã§åŠ ç®—/æ¸›ç®—
    c = tl.where(w == 2, x, tl.where(w == 0, -x, 0.0))
    acc += c
```

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:**

| Config | Current (tl.dot) | LUT | Ternary |
|--------|-----------------|-----|---------|
| (1, 4096, 4096) | 0.48 ms | 0.48 ms | 0.70 ms |
| (32, 4096, 4096) | 0.48 ms | 0.48 ms | 0.73 ms |

**çµè«–**: GPU ã§ã¯ LUT æ–¹å¼ã®ãƒ¡ãƒªãƒƒãƒˆãŒãªã„ã€‚

### ãªãœ GPU ã§ã¯åŠ¹æœãŒãªã„ã®ã‹

| è¦ç´  | CPU | GPU |
|------|-----|-----|
| è¡Œåˆ—æ¼”ç®— | SIMD FMA | **Tensor Core** |
| LUT é…ç½® | L1 ã‚­ãƒ£ãƒƒã‚·ãƒ¥ | Shared Memory |
| é«˜é€Ÿå‘½ä»¤ | vpshufb (shuffle) | ãªã— |

GPU ã® Tensor Core ã¯ FP16/FP32 ã®è¡Œåˆ—ç©ã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€LUT ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚ˆã‚Šåœ§å€’çš„ã«é«˜é€Ÿã§ã™ã€‚

## å®Ÿé¨“3: å®Ÿç”¨æ€§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡

| ãƒ¢ãƒ‡ãƒ« | FP16 | BitNet | åœ§ç¸®ç‡ |
|--------|------|--------|-------|
| GPT-2 | 0.11 GB | 0.01 GB | 16x |
| LLaMA-7B | 8.00 GB | 0.50 GB | 16x |

### ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (hidden=4096)

| Batch Size | FP16 (tok/s) | BitNet (tok/s) | æ¯”ç‡ |
|------------|--------------|----------------|------|
| 1 | 11,087 | 2,136 | 0.19x |
| 32 | 345,879 | 68,919 | 0.20x |
| 512 | 1,997,117 | 237,756 | 0.12x |

### æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º

```
FP16:   2,048 ã‚µãƒ³ãƒ—ãƒ«
BitNet: 8,192 ã‚µãƒ³ãƒ—ãƒ« (4x å¤šã„ï¼)
```

## BitNet ã®ä½¿ã„ã©ã“ã‚

### âœ“ åŠ¹æœçš„ãªã‚±ãƒ¼ã‚¹

| ã‚·ãƒŠãƒªã‚ª | ç†ç”± |
|---------|------|
| **ãƒ¡ãƒ¢ãƒªä¸è¶³** | 16x åœ§ç¸®ã§å¤§ãƒ¢ãƒ‡ãƒ«ãŒå‹•ã |
| **å¤§é‡ãƒãƒƒãƒæ¨è«–** | åŒãƒ¡ãƒ¢ãƒªã§ 4x å¤šãã®ã‚µãƒ³ãƒ—ãƒ« |
| **ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹** | å°ã•ã„ GPU ã§å¤§ãã„ãƒ¢ãƒ‡ãƒ« |
| **ã‚³ã‚¹ãƒˆå‰Šæ¸›** | å°ã•ã„ GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ |

### âœ— ä¸å‘ããªã‚±ãƒ¼ã‚¹

| ã‚·ãƒŠãƒªã‚ª | ç†ç”± |
|---------|------|
| **ãƒ¡ãƒ¢ãƒªã«ä½™è£•ã‚ã‚Š** | FP16 ã®æ–¹ãŒé€Ÿã„ |
| **ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¿…é ˆ** | å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã¯ 0.2x |
| **æœ€é«˜å“è³ªãŒå¿…è¦** | é‡å­åŒ–ã«ã‚ˆã‚‹ç²¾åº¦ä½ä¸‹ |

## GPU ã§ã®é«˜é€ŸåŒ–: BitNet ä»¥å¤–ã®é¸æŠè‚¢

BitNet ã¯ã€Œãƒ¡ãƒ¢ãƒªã€ãŒç›®çš„ã§ã€Œé€Ÿåº¦ã€ã¯çŠ ç‰²ã«ãªã‚Šã¾ã™ã€‚é€Ÿåº¦é‡è¦–ãªã‚‰:

| æ‰‹æ³• | åœ§ç¸® | é€Ÿåº¦ | ç”¨é€” |
|------|------|------|------|
| FP16 + Flash Attention | 2x | 2-4x | æ±ç”¨ |
| INT8 + TensorRT | 4x | 2-4x | æ¨è«–æœ€é©åŒ– |
| INT4 (AWQ/GPTQ) | 8x | 1.5-2x | ãƒ¡ãƒ¢ãƒªç¯€ç´„ |
| vLLM | - | 2-24x | LLM ã‚µãƒ¼ãƒ“ãƒ³ã‚° |
| Speculative Decoding | - | 2-3x | ç”Ÿæˆé«˜é€ŸåŒ– |

## ã¾ã¨ã‚

### å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹

1. **BitNet ã®æœ¬è³ªã¯ãƒ¡ãƒ¢ãƒªåœ§ç¸®**
   - GPU ã§ã¯é€Ÿåº¦å‘ä¸Šã—ãªã„ï¼ˆTensor Core ãŒå¼·ã™ãã‚‹ï¼‰
   - CPU ã§ã¯ T-MAC/LUT æ–¹å¼ã§é€Ÿåº¦ã‚‚å‘ä¸Š

2. **æ¡ä»¶ä»˜ã‘å±¤ã¯é‡å­åŒ–ã—ãªã„**
   - æ™‚é–“åŸ‹ã‚è¾¼ã¿ã€ã‚¯ãƒ©ã‚¹åŸ‹ã‚è¾¼ã¿ã¯ FP32 ã‚’ç¶­æŒ
   - Attentionã€MLP ã¯ BitLinear OK

3. **LUT æ–¹å¼ã¯ CPU å°‚ç”¨**
   - GPU ã® Tensor Core > LUT ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—
   - CPU ã® SIMD shuffle ã¯ LUT ã«æœ€é©

4. **å®Ÿç”¨çš„ãªä¾¡å€¤**
   - ãƒ¡ãƒ¢ãƒª 16x åœ§ç¸® â†’ å¤§ãƒ¢ãƒ‡ãƒ«ãŒå‹•ã
   - ãƒãƒƒãƒã‚µã‚¤ã‚º 4x â†’ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š
   - é€Ÿåº¦ã¯ 0.2x â†’ ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

### çµè«–

BitNet ã¯ã€Œé€Ÿåº¦ã¨å¼•ãæ›ãˆã«ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã™ã‚‹ã€æŠ€è¡“ã§ã™ã€‚

```
LLaMA-7B:
  FP16: 26GB â†’ å¤§ãã„ GPU å¿…è¦
  BitNet: 1.6GB â†’ 4GB GPU ã§å‹•ä½œå¯èƒ½ï¼
```

ãƒ¡ãƒ¢ãƒªãŒåˆ¶ç´„ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãªã‚‰ã€BitNet ã¯ã€Œå‹•ã‹ã›ãªã„ã€ã‚’ã€Œå‹•ã‹ã›ã‚‹ã€ã«å¤‰ãˆã‚‹å¼·åŠ›ãªé¸æŠè‚¢ã§ã™ã€‚

## å‚è€ƒè³‡æ–™

- [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453)
- [The Era of 1-bit LLMs (BitNet b1.58)](https://arxiv.org/abs/2402.17764)
- [BitNet.cpp](https://github.com/microsoft/BitNet)
- [T-MAC: Table Lookup for Ternary Matrix Multiplication](https://arxiv.org/abs/2407.00088)
